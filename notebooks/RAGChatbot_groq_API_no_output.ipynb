{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgaida/llm_client/blob/master/notebooks/RAGChatbot_groq_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILap3b9lJLaJ"
      },
      "outputs": [],
      "source": [
        "# Chatbot using Retrieval Augmented Generation\n",
        "# Uses Groq Inference Platform to run LLM online\n",
        "\n",
        "# TODO: benötigt groq API Key in GROQ_API_KEY und HuggingFace Token in HF_TOKEN (für Embedding Modell)\n",
        "# TODO: erstelle Ordner pdfs und lade ein oder mehrere PDFs in diesen Ordner hoch\n",
        "\n",
        "# IMPORTANT: run this cell first, then a message appears that you have to restart the session.\n",
        "# in the notebook click on \"restart session\". Afterwards run all other cells.\n",
        "# DO NOT RUN all cells immediately after opening the notebook! Run the first cell only. Then the rest.\n",
        "\n",
        "# install packages\n",
        "!apt-get -qq install -y poppler-utils tesseract-ocr libmagic1\n",
        "!pip install \"unstructured[all-docs]\" python-magic llama-index==0.12.35 llama-index-vector-stores-chroma llama-index-embeddings-huggingface chromadb pdf2image pytesseract gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgkidGD7sx5f"
      },
      "outputs": [],
      "source": [
        "# install package that lets you use Groq or OpenAI\n",
        "!git clone https://github.com/dgaida/llm_client.git\n",
        "%cd llm_client\n",
        "!pip install .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSnf0xfaJfoH"
      },
      "outputs": [],
      "source": [
        "# --- Import required libraries ---\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "import gradio as gr\n",
        "from llm_client import LLMClient, LLMClientAdapter\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/llm_client/notebooks\")\n",
        "\n",
        "import utils\n",
        "\n",
        "# --- Step 1: Set up PDF file path ---\n",
        "# Put your PDF files in a folder named \"pdfs\" in the current directory\n",
        "PDF_DIR = \"pdfs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jm4qpmjJint"
      },
      "outputs": [],
      "source": [
        "# --- Step 2: Load and parse PDFs with uMiner (via UnstructuredReader) ---\n",
        "all_documents = utils.read_pdf_files_with_unstructured_reader(PDF_DIR)\n",
        "\n",
        "print(f\"Loaded {len(all_documents)} documents\")\n",
        "for doc in all_documents[:3]:\n",
        "    print(doc.text[:300])  # Zeige ersten Ausschnitt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neodti2PYp0C"
      },
      "outputs": [],
      "source": [
        "# split documents into smaller chunks for better retrieval\n",
        "node_parser = SentenceSplitter(chunk_size=256, chunk_overlap=0)  # 512, 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7eY260BsWk6"
      },
      "outputs": [],
      "source": [
        "# this is the embedding model\n",
        "# models can be found here: https://huggingface.co/spaces/mteb/leaderboard\n",
        "embed_model = HuggingFaceEmbedding(model_name = \"intfloat/e5-small-v2\")\n",
        "#embed_model = HuggingFaceEmbedding(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvatDMmVRN13"
      },
      "outputs": [],
      "source": [
        "# client that uses openAI or Grog API, here groq API\n",
        "client = LLMClient(api_choice = \"groq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4jtwTq8rqVo"
      },
      "outputs": [],
      "source": [
        "# Adapter erzeugen\n",
        "adapter = LLMClientAdapter(client=client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhqg9ldfaxzr"
      },
      "outputs": [],
      "source": [
        "# Apply settings globally - LlamaIndex uses this Settings object internally\n",
        "Settings.llm = adapter  # None\n",
        "Settings.embed_model = embed_model\n",
        "Settings.node_parser = node_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nkoYBBMJm1T"
      },
      "outputs": [],
      "source": [
        "# --- Step 4: Create vector store and index ---\n",
        "index = utils.create_chromadb_vector_store_and_index(all_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNZJHplkSGwk"
      },
      "outputs": [],
      "source": [
        "# --- Step 5: Create query engine ---\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOZ_vaKpJpmH"
      },
      "outputs": [],
      "source": [
        "# this function does everyting.\n",
        "# takes the query of the user and embeds it into the vector space,\n",
        "# does a semantic search in the vector database (compares vectors)\n",
        "# returns the text of the most similar vector\n",
        "# passes this text as context to the LLM together with the query\n",
        "# retrieves the response of the LLM and returns it as a string\n",
        "# llama_index does this all internally using the Settings object.\n",
        "def chat_with_pdf(query, history=None):\n",
        "    return utils.safe_query_engine_call(query_engine, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4De1B1sdU0Mz"
      },
      "outputs": [],
      "source": [
        "print(chat_with_pdf(\"Welche Module gibt es in dem Studiengang?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xCZPR-PUwki"
      },
      "outputs": [],
      "source": [
        "# --- Step 7: Build Gradio Interface ---\n",
        "# This creates a simple web UI for asking questions\n",
        "chat_ui = gr.ChatInterface(\n",
        "    fn=chat_with_pdf,\n",
        "    title=\"PDF RAG Chatbot\",\n",
        "    description=\"Ask questions about the content of your PDF documents.\",\n",
        "    theme=\"default\",\n",
        "    examples=[\"What is this PDF about?\", \"Summarize the second section.\"],\n",
        ")\n",
        "\n",
        "chat_ui.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}