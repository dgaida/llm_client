# ğŸ§  LLM Client

Ein universeller Python-Client zur Nutzung verschiedener Large Language Models (LLMs)
Ã¼ber **OpenAI**, **Groq** oder **Ollama** â€“ mit automatischer API-Erkennung.

---

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
[![Tests](https://github.com/dgaida/llm_client/actions/workflows/tests.yml/badge.svg)](https://github.com/dgaida/llm_client/actions/workflows/tests.yml)
[![Code Quality](https://github.com/dgaida/llm_client/actions/workflows/lint.yml/badge.svg)](https://github.com/dgaida/llm_client/actions/workflows/lint.yml)
[![CodeQL](https://github.com/dgaida/llm_client/actions/workflows/codeql.yml/badge.svg)](https://github.com/dgaida/llm_client/actions/workflows/codeql.yml)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

## ğŸ“‘ Inhaltsverzeichnis

- [Features](#-features)
- [Installation](#%EF%B8%8F-installation)
- [Schnellstart](#-schnellstart)
- [Verwendung](#-verwendung)
- [API UnterstÃ¼tzung](#-unterstÃ¼tzte-apis--default-modelle)
- [Tests](#-tests-ausfÃ¼hren)
- [Contributing](#-contributing)
- [Lizenz](#-lizenz)

## ğŸš€ Features

* ğŸ” **Automatische API-Erkennung** - Nutzt verfÃ¼gbare API-Keys oder fÃ¤llt auf Ollama zurÃ¼ck
* âš™ï¸ **Einheitliches Interface** - Eine Methode fÃ¼r alle LLM-Backends
* ğŸ§© **Flexible Konfiguration** - Modell, Temperatur, Tokens frei wÃ¤hlbar
* ğŸ§ª **VollstÃ¤ndige Tests** - Pytest-basiert mit hoher Code-Coverage
* ğŸ” **Google Colab Support** - Automatisches Laden von Secrets aus userdata
* ğŸ“¦ **Zero-Config** - Funktioniert out-of-the-box mit Ollama

---

## âš™ï¸ Installation

### Schnellinstallation

```bash
pip install git+https://github.com/dgaida/llm_client.git
```

### Entwicklungsinstallation

```bash
git clone https://github.com/dgaida/llm_client.git
cd llm_client
pip install -e ".[dev]"
```

### Mit llama-index Support

```bash
pip install -e ".[llama-index]"
```

---

## ğŸš¦ Schnellstart

```python
from llm_client import LLMClient

# Automatische API-Erkennung
client = LLMClient()

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "ErklÃ¤re Machine Learning in einem Satz."}
]

response = client.chat_completion(messages)
print(response)
```

---

## ğŸ”§ Konfiguration

### API-Keys einrichten

Erstellen Sie `secrets.env`:

```bash
# OpenAI
OPENAI_API_KEY=sk-xxxxxxxx

# Oder Groq
GROQ_API_KEY=gsk-xxxxxxxx
```

**Ohne API-Keys**: Verwendet automatisch lokales Ollama (Installation erforderlich).

### Google Colab

In Colab werden Keys automatisch aus `userdata` geladen:

```python
# Secrets â†’ OPENAI_API_KEY oder GROQ_API_KEY hinzufÃ¼gen
from llm_client import LLMClient
client = LLMClient()  # LÃ¤dt automatisch aus userdata
```

---

## ğŸ“š Erweiterte Verwendung

### Spezifisches Modell wÃ¤hlen

```python
client = LLMClient(
    llm="gpt-4o",
    temperature=0.5,
    max_tokens=2048
)
```

### API manuell wÃ¤hlen

```python
# Ollama erzwingen (auch wenn API-Keys vorhanden)
client = LLMClient(api_choice="ollama")

# OpenAI explizit wÃ¤hlen
client = LLMClient(api_choice="openai", llm="gpt-4")
```

### Mit llama-index Integration

```python
from llm_client import LLMClientAdapter, LLMClient

# Adapter erstellen
llm_adapter = LLMClientAdapter(client=LLMClient())

# In llama-index verwenden
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents, llm=llm_adapter)
```

---

## ğŸ§© UnterstÃ¼tzte APIs & Default-Modelle

| API    | Default-Modell                     | Bemerkung                       |
| ------ | ---------------------------------- | ------------------------------- |
| OpenAI | `gpt-4o-mini`                      | Schnell, zuverlÃ¤ssig            |
| Groq   | `moonshotai/kimi-k2-instruct-0905` | Sehr effizient auf GroqCloud    |
| Ollama | `llama3.2:1b`                      | LÃ¤uft lokal, kein API-Key nÃ¶tig |

### Ollama Installation

```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download von https://ollama.ai/download

# Modell herunterladen
ollama pull llama3.2:1b
```

---

## ğŸ§ª Tests ausfÃ¼hren

```bash
# Alle Tests
pytest

# Mit Coverage Report
pytest --cov=llm_client --cov-report=html

# Einzelne Tests
pytest tests/test_llm_client.py -v
```

### Code-QualitÃ¤t prÃ¼fen

```bash
# Formatierung
black .

# Linting
ruff check .

# Auto-fix
ruff check --fix .
```

---

## ğŸ‘¥ Contributing

BeitrÃ¤ge sind willkommen! Siehe [CONTRIBUTING.md](CONTRIBUTING.md) fÃ¼r Details.

### Entwickler-Workflow

1. Fork & Clone
2. Feature-Branch erstellen: `git checkout -b feature/mein-feature`
3. Tests schreiben und ausfÃ¼hren
4. Code formatieren: `black . && ruff check --fix .`
5. Commit & Push
6. Pull Request Ã¶ffnen

---

## ğŸ“Š Projektstruktur

```
llm_client/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/         # CI/CD Pipelines
â”œâ”€â”€ llm_client/
â”‚   â”œâ”€â”€ __init__.py       # Package Exports
â”‚   â”œâ”€â”€ llm_client.py     # Hauptklasse
â”‚   â””â”€â”€ adapter.py        # llama-index Integration
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_llm_client.py
â”œâ”€â”€ main.py               # Beispiel-Script
â”œâ”€â”€ pyproject.toml        # Dependencies & Config
â”œâ”€â”€ README.md
â”œâ”€â”€ CONTRIBUTING.md
â””â”€â”€ LICENSE
```

---

## ğŸ“„ Lizenz

MIT License - siehe [LICENSE](LICENSE)

Â© 2025 Daniel Gaida, Technische Hochschule KÃ¶ln

---

## ğŸ”— WeiterfÃ¼hrende Links

* [Ollama Dokumentation](https://github.com/ollama/ollama)
* [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
* [Groq Cloud](https://groq.com/)
* [llama-index Docs](https://docs.llamaindex.ai/)

---

## â­ Support

Wenn Ihnen dieses Projekt gefÃ¤llt, geben Sie ihm einen Stern auf GitHub!

Fragen? Ã–ffnen Sie ein [Issue](https://github.com/dgaida/llm_client/issues).
