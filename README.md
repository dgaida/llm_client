# ğŸ§  LLM Client

Ein universeller Python-Client zur Nutzung verschiedener Large Language Models (LLMs)
Ã¼ber **OpenAI**, **Groq** oder **Ollama** â€“ mit automatischer API-Erkennung anhand von `secrets.env`.

---

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
[![Tests](https://github.com/dgaida/llm_client/actions/workflows/tests.yml/badge.svg)](https://github.com/dgaida/llm_client/actions/workflows/tests.yml)
[![Code Quality](https://github.com/dgaida/llm_client/actions/workflows/lint.yml/badge.svg)](https://github.com/dgaida/llm_client/actions/workflows/lint.yml)
[![CodeQL](https://github.com/dgaida/llm_client/actions/workflows/codeql.yml/badge.svg)](https://github.com/dgaida/llm_client/actions/workflows/codeql.yml)

## ğŸš€ Features

* ğŸ” **Automatische API-Erkennung**
  * Nutzt `OPENAI_API_KEY` oder `GROQ_API_KEY`, falls vorhanden.
  * FÃ¤llt automatisch auf **Ollama** zurÃ¼ck, wenn keine API-Keys gefunden werden.
* âš™ï¸ **Einheitliches Interface**
  * Eine Methode `chat_completion(messages)` fÃ¼r alle Backends.
* ğŸ§© **Flexible Konfiguration**
  * Modell, Temperatur, Token-Limit und API-Typ frei wÃ¤hlbar.
* ğŸ§ª **Testabdeckung**
  * Pytest-basiertes Testsuite inklusive Mocking und Fehlerhandling.

---

## ğŸ§± Projektstruktur

```
llm-client/
â”‚
â”œâ”€â”€ llm_client/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ llm_client.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_llm_client.py
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ secrets.env
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ environment.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE                        # MIT Lizenz
â””â”€â”€ .gitignore                     # Git-Ignore-Regeln
```

---

## âš™ï¸ Installation

### ğŸ§  1. Umgebung erstellen

Mit Conda:

```bash
conda env create -f environment.yml
conda activate llm-client-env
```

oder mit Mamba:

```bash
mamba env create -f environment.yml
mamba activate llm-client-env
```

---

### ğŸª„ 2. Installation im Entwicklungsmodus

```bash
pip install -e .[dev]
```

Dies installiert:

* `llm_client` (das eigentliche Package)
* `pytest`, `ruff`, `pytest-cov` (fÃ¼r Tests & Linting)

---

### ğŸ”‘ 3. API-Keys konfigurieren

Lege eine Datei `secrets.env` im Projektverzeichnis an:

```bash
OPENAI_API_KEY=sk-xxxxxxxx
# GROQ_API_KEY=groq-xxxxxxxx
```

Wenn keine Keys gesetzt sind, verwendet der Client automatisch **Ollama**.
ğŸ”— Stelle sicher, dass Ollama lokal installiert ist.

[Ollama Python Download](https://www.ollama.com/download)

---

## ğŸ’¡ Verwendung

### Beispiel `main.py`

```python
from llm_client import LLMClient

def main():
    client = LLMClient()  # automatische API-Erkennung

    print(f"Verwendete API: {client.api_choice}")
    print(f"Verwendetes Modell: {client.llm}")

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "ErklÃ¤re kurz, was neuronale Netze sind."}
    ]

    response = client.chat_completion(messages)
    print("\nAntwort:\n", response)

if __name__ == "__main__":
    main()
```

AusfÃ¼hren mit:

```bash
python main.py
```

---

## ğŸ§¢ Tests ausfÃ¼hren

```bash
pytest
```

Mit Code Coverage:

```bash
pytest --cov=llm_client
```

---

## âš¡ï¸ Beispiele fÃ¼r Konfiguration

### API explizit wÃ¤hlen

```python
client = LLMClient(api_choice="ollama")
```

### Modell und Temperatur Ã¤ndern

```python
client = LLMClient(llm="gpt-4o", temperature=0.5)
```

### Tokens und Keep-Alive anpassen

```python
client = LLMClient(max_tokens=2048, keep_alive="10m")
```

---

## ğŸ§© UnterstÃ¼tzte APIs & Default-Modelle

| API    | Default-Modell                 | Bemerkung                       |
| ------ | ------------------------------ | ------------------------------- |
| OpenAI | `gpt-4o-mini`                  | Schnell, zuverlÃ¤ssig            |
| Groq   | `moonshotai/kimi-k2-instruct-0905` | Sehr effizient auf GroqCloud    |
| Ollama | `llama3.2:1b`                     | LÃ¤uft lokal, kein API-Key nÃ¶tig |

---

## ğŸ‘¨â€ğŸ’» Entwickler:innen-Info

**Tests & Linting**

```bash
pytest
ruff check llm_client
```

**Version bump (manuell):**

```bash
# in pyproject.toml Ã¤ndern:
version = "0.1.1"
```

---

## ğŸ“„ Lizenz

Dieses Projekt steht unter der **MIT-Lizenz**.
Â© 2025 â€“ Daniel Gaida, Technische Hochschule KÃ¶ln.

---

## ğŸŒ Hinweise

* [Ollama Python API Doku](https://github.com/ollama/ollama/tree/main/api)
* [OpenAI Python SDK](https://github.com/openai/openai-python)
* [Groq SDK](https://github.com/groq/groq-python)
